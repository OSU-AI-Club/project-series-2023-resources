{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **BEFORE YOU START**\n",
        "\n",
        "*  Click on the dropdown arrow next to the \"RAM DISK\" metric on the top right\n",
        "*  Click \"change runtime type\"\n",
        "*  Select \"T4 GPU\"\n",
        "\n",
        "You can run this without doing these steps, but it will be very slow\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4m2cZGuUGzH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import necessary libraries"
      ],
      "metadata": {
        "id": "kDTaqem3Edyz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sbXa4_hKnh0h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, save, load\n",
        "from torch.optim import Adam\n",
        "import torchvision\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHNRpBSspIQm"
      },
      "source": [
        "# Data Loading & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2uHtuzlKo_Hz"
      },
      "outputs": [],
      "source": [
        "#this is for data preprocessing and loading with train data\n",
        "def train_pl():\n",
        "    #the transformation we will apply to the images from the FER2013 dataset\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Grayscale(),\n",
        "        transforms.ToTensor(), # Convert image to tensor\n",
        "        transforms.Normalize(0.485, 0.229) # Normalize image\n",
        "    ])\n",
        "\n",
        "    # loading the data from the directory I have stored the downloaded FER2013 dataset\n",
        "    train_data = torchvision.datasets.FER2013(root='/content/dataset', split = 'train', transform=transform)\n",
        "    print(f\"Length of train data: {len(train_data)}\")\n",
        "    # create dataloaders so that the FER2013 data can be loaded into the model we will implement\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=19, shuffle=True, num_workers=2)\n",
        "\n",
        "    return train_loader\n",
        "\n",
        "#this is for data preprocessing and loading with test data\n",
        "def test_pl():\n",
        "    #the transformation we will apply to the images from the FER2013 dataset\n",
        "    transform = transforms.Compose([\n",
        "        # transforms.Grayscale(),\n",
        "        transforms.ToTensor(), # Convert image to tensor\n",
        "        transforms.Normalize(0.485, 0.229) # Normalize image\n",
        "    ])\n",
        "\n",
        "    # loading the data from the directory I have stored the downloaded FER2013 dataset\n",
        "    test_data = torchvision.datasets.FER2013(root='/content/dataset', split = 'test' ,  transform=transform)\n",
        "    print(f\"Length of test data: {len(test_data)}\")\n",
        "    # create dataloaders so that the FER2013 data can be loaded into the model we will implement\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "    return test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3OeW03NqyVb"
      },
      "source": [
        "# Designing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "D8ATd0ScpOWp"
      },
      "outputs": [],
      "source": [
        "class EmotionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EmotionModel, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(128 * 6 * 6, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 7)  # 7 classes for different emotions\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK-mzU2orFbe"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OOmb0uFrrChD"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    model = EmotionModel().to('cuda')\n",
        "    optimizer = Adam(model.parameters(), lr = 1e-3)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_set = train_pl()\n",
        "\n",
        "    for epoch in range(50): #train for 50 epochs\n",
        "        for batch in train_set:\n",
        "            X, y = batch\n",
        "            X, y = X.to('cuda'), y.to('cuda')\n",
        "            prediction = model(X)\n",
        "            loss = loss_fn(prediction, y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "        print(f\"\\tloss:{loss}\")\n",
        "        print(\"--------------------------------------------\")\n",
        "\n",
        "    # saving our model to our environment\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioP7EFKgsI_L"
      },
      "source": [
        "# Tying everything together to have a savable model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnVXhq_QsLxt"
      },
      "outputs": [],
      "source": [
        "    trained_model = train()\n",
        "    save(trained_model, 'model_MK1')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to run the model on images\n",
        "### **Don't worry about this code. You aren't required to understand or learn this for the purposes of making a deep learning model. This is just for demonstration of the model**"
      ],
      "metadata": {
        "id": "2xWffDh1_4PX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyKnqsRh5yym",
        "outputId": "858308fe-b065-45cc-da17-f1c4c768a105"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mtcnn\n",
            "  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (2.12.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python>=4.1.0->mtcnn) (1.23.5)\n",
            "Installing collected packages: mtcnn\n",
            "Successfully installed mtcnn-0.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install mtcnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hp0TpuBR4wey"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from mtcnn import MTCNN\n",
        "from torchvision import transforms\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def run_model(input_image_path, output_image_path):\n",
        "  # Load trained model\n",
        "  model = torch.load('/content/model_MK1')\n",
        "  model.eval()\n",
        "  model.to(torch.device('cuda'))\n",
        "  model = torch.jit.script(model)\n",
        "\n",
        "  # Load emotion labels\n",
        "  emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "  # Load and preprocess the input image\n",
        "  input_image = cv2.imread(input_image_path)\n",
        "  gray_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # Initialize MTCNN for face detection\n",
        "  mtcnn = MTCNN()\n",
        "\n",
        "  # Detect faces in the image\n",
        "  faces = mtcnn.detect_faces(input_image)\n",
        "\n",
        "  for face_info in faces:\n",
        "      x, y, w, h = [int(coord) for coord in face_info['box']]\n",
        "      face = gray_image[y:y + h, x:x + w]\n",
        "\n",
        "      # Preprocess the face image\n",
        "      face = cv2.resize(face, (48, 48))\n",
        "      face_tensor = transforms.ToTensor()(face).unsqueeze(0).to(torch.device('cuda'))\n",
        "\n",
        "      with torch.no_grad():\n",
        "          predictions = model(face_tensor)\n",
        "      predicted_emotion = emotion_labels[predictions.argmax()]\n",
        "\n",
        "      cv2.rectangle(input_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "      cv2.putText(input_image, predicted_emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "  # Display or save the output image\n",
        "  cv2.imwrite(output_image_path, input_image)\n",
        "  cv2_imshow(input_image)\n",
        "  cv2.waitKey(0)\n",
        "  cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJLxpwX94sy6"
      },
      "source": [
        "## Run the model on a screenshot of **happy** face stock images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_model('/content/test_images/happy_test.png', '/content/results/happy_result.png')"
      ],
      "metadata": {
        "id": "yxhhXirhAPgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8hcGo9q_ASt"
      },
      "source": [
        "## Run the model on a screenshot of **sad** face stock images\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_model('/content/test_images/sad_test.png', '/content/results/sad_result.png')"
      ],
      "metadata": {
        "id": "JL197odxCMy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUpjpnVl_Fhz"
      },
      "source": [
        "## Run the model on a screenshot of **angry** face stock images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_model('/content/test_images/angry_test.png', '/content/results/angry_result.png')"
      ],
      "metadata": {
        "id": "0N1hS-mtCd0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzdMfGcr_GAc"
      },
      "source": [
        "## Run the model on a screenshot of **fear** face stock images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_model('/content/test_images/fear_test.png', '/content/results/fear_result.png')"
      ],
      "metadata": {
        "id": "Ez8qGPOaCwyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jV_3HR9_HHS"
      },
      "source": [
        "## Run the model on a screenshot of **surprised** face stock images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_model('/content/test_images/surprised_test.png', '/content/results/surprised_result.png')"
      ],
      "metadata": {
        "id": "CL8wli5lCzsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk2j6jRW_b70"
      },
      "source": [
        "## Run the model on a screenshot of **disgusted** face stock images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_model('/content/test_images/disgusted_test.png', '/content/results/disgusted_result.png')"
      ],
      "metadata": {
        "id": "55EOW7TeC3LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO7hXmLX_HRZ"
      },
      "source": [
        "## Run the model on a screenshot of **neutral** face stock images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_model('/content/test_images/neutral_test.png', '/content/results/neutral_result.png')"
      ],
      "metadata": {
        "id": "mY4L002C_lUO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}